{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Network & Atari\n",
        "*Prepared by Yashwanthi Anand,*\n",
        "\n",
        "For this section, we follow the implementations available at [CleanRL](https://github.com/vwxyzjn/cleanrl).\n",
        "\n",
        "CleanRL is an open-source library designed for clean, single-file implementations of RL algorithms, such as DQN, PPO, and A2C. CleanRL's simple implementation, makes it accessible for newcomers to RL while still being robust for research."
      ],
      "metadata": {
        "id": "5pHyDGDiomFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with CleanRL\n",
        "\n",
        "There are two ways to go about experimenting with the CleanRL ‚Äì (1) Clone it to your local machine and run the code, or (2) Use an environment prebuilt of Gitpod (Note, you will have to authorize integrating Gitpod with GitHub)\n",
        "\n",
        "\n",
        "\n",
        "## 1.   Cloning it to your local machine\n",
        "\n",
        "Please follow the installation and steps to get started with CleanRL [here](https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file#get-started)\n",
        "\n",
        "\n",
        "In this session, as we focus only on running DQN on Atari, it suffices if you install the following,\n",
        "```\n",
        "pip install -r requirements/requirements.txt\n",
        "pip install -r requirements/requirements-atari.txt\n",
        "```\n",
        "\n",
        "Alternatively, you can use `poetry` to install all the dependencies.\n",
        "\n",
        "## 2.   Hosting on Gitpod\n",
        "\n",
        "Please click [here](https://gitpod.io/#https://github.com/vwxyzjn/cleanrl) to launch the repository on Gitpod. This will load the entire repository.\n",
        "Note, if you are new to Gitpod, you will have to authorize integrating Gitpod with GitHub.)\n",
        "\n",
        "In this session, as we focus only on running DQN on Atari, it suffices if you install the following,\n",
        "```\n",
        "pip install -r requirements/requirements.txt\n",
        "pip install -r requirements/requirements-atari.txt\n",
        "```\n",
        "\n",
        "Alternatively, you can use `poetry` to install all the dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "29Rx9UBfsMxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Implementation\n",
        "\n",
        "The file we will be using is located at `cleanrl/dqn_atari.py`\n",
        "\n",
        "### To run the code\n",
        "Use `python cleanrl/dqn_atari.py --env-id <ENVIRONMENT_NAME>`.\n",
        "For example, if you want to run the DQN code for Atari Breakout use `python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4`.\n",
        "### To run the code using `poetry`\n",
        "```\n",
        "poetry shell\n",
        "python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4\n",
        "```"
      ],
      "metadata": {
        "id": "iOSBfe9HDorm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights and Biases\n",
        "\n",
        "CleanRL uses [Weights and Biases](https://wandb.ai/site) `wandb` for experiment tracking and model monitoring. `wandb` allows users to log and visualize metrics, hyperparameters, and outputs in real-time.\n",
        "\n",
        "Please refer to [this page](https://docs.cleanrl.dev/get-started/experiment-tracking/) to learn how to set up `wandb`\n",
        "\n"
      ],
      "metadata": {
        "id": "y-3bWqjPJlCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudocode\n",
        "\n",
        "\n",
        "\n",
        "1.   Take action a<sub>t</sub> with Œµ-probability\n",
        "2.   Store transitions (s<sub>t</sub>,a<sub>t</sub>,r<sub>t+1</sub>,s<sub>t+1</sub>) in replay buffer D\n",
        "3. Sample random mini-batch of transitions (s,a,r,s‚Äô) from D\n",
        "4. Compute Q-learning targets w.r.t. old, fixed weights of the target network **w<sup>-</sup>**\n",
        "5. Optimize MSE between Q-network & Q-learning targets \\\\\n",
        "J(**w<sub>i</sub>**) = ùêÑ<sub>s,a,r,s‚Äô~D</sub>[((r + ùõÑ max a‚Äô Q(s‚Äô, a‚Äô; **w<sup>-</sup>**) ‚Äì Q(s, a; **w<sub>i</sub>**))<sup>2</sup>]\n",
        "6. Update **w** using stochastic gradient descent\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vwzxdfjl3iN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Code\n",
        "\n",
        "The details of [CleanRL's DQN implementation](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py) is provided in the following code snippets."
      ],
      "metadata": {
        "id": "d7x_XmYNGNFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN uses two networks ‚Äì Q Network and the Target Network. The following cell provides the network structure for them both."
      ],
      "metadata": {
        "id": "GC4j42vSH5eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Lines 106-124\n",
        "\n",
        "# ALGO LOGIC: initialize agent here:\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.single_action_space.n),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x / 255.0)```\n",
        "\n"
      ],
      "metadata": {
        "id": "_qZyWdMa5kEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Lines 177-189\n",
        "\n",
        "\"\"\"Initialize the Q Network and the Target Network\"\"\"\n",
        "q_network = QNetwork(envs).to(device)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
        "target_network = QNetwork(envs).to(device)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "\n",
        "\"\"\"Initialize the replay buffer to keep a record of (s,a,r',s') tuples\"\"\"\n",
        "rb = ReplayBuffer(\n",
        "    args.buffer_size,\n",
        "    envs.single_observation_space,\n",
        "    envs.single_action_space,\n",
        "    device,\n",
        "    optimize_memory_usage=True,\n",
        "    handle_timeout_termination=False,\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wQeRVgC96GKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# STEP 1: Action Selection - Q Network selects an action with epsilon probability\n",
        "# Lines 196-201\n",
        "# ALGO LOGIC: put action logic here\n",
        "\n",
        "epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
        "if random.random() < epsilon:\n",
        "    actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "else:\n",
        "    q_values = q_network(torch.Tensor(obs).to(device))\n",
        "    actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WTjNKU976X6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Step 2: Store transitions in replay buffer\n",
        "# Line 219\n",
        "\n",
        "rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rLnp36Su6_TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Steps 3-4\n",
        "# Lines 225-232\n",
        "\n",
        "if global_step > args.learning_starts:\n",
        "    if global_step % args.train_frequency == 0:\n",
        "        data = rb.sample(args.batch_size)\n",
        "        with torch.no_grad():\n",
        "            target_max, _ = target_network(data.next_observations).max(dim=1)\n",
        "            td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
        "        old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
        "        loss = F.mse_loss(td_target, old_val)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "g6i_UfPI7M7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Step 5: Optimize\n",
        "# Lines 241-243\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ooLq1-ok8G69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Step 6: Update Target network\n",
        "# Line 246-250\n",
        "\n",
        "if global_step % args.target_network_frequency == 0:\n",
        "    for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "        target_network_param.data.copy_(\n",
        "            args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
        "        )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YEV_DzKa74HN"
      }
    }
  ]
}